[TOC]



## 依赖知识

a) 了解神经网络的基础知识

b) 熟悉导数的链式法则及常见函数的导数

c) 熟悉常见的优化方法，梯度下降，随机梯度下降等

d) 熟悉矩阵和向量的乘加运算



## 约定说明

a) 对于一个$n$层神经网络,第$i$层表示为$l_i, \  1 \le i \le n$ ,第$i$神经元个数为$|l_i|$ ; 注意$l_i$是输入层, $l_n$ 是输出层。



b) 对于神经网络的一个输入样本$x$ ,其维度为$(1,|l_1|)$ ,也就是一个行向量；对应的输出$y$ 也是一个行向量，维度为$(1,|l_n|)$ 。(注：也可以将$x$ 和 $y$ 表示为列向量，这里预先说明好为行向量，避免后面有歧义)



c) 设神经网络中第i层的输出为$z^i$,($z^i$ 都是行向量)则 $x=z^1, y=z^n$ ; 第$i$ 层的权重和偏置分别为$W^i, b^i$ ;则对于全连接层$z^{i+1} = z^iW^i + b^i$ ; 其中$W^i$ 和$b^i$的维度分别为为$(|l_i|,|l_{i+1}|),(1,|l_{i+1}|)$ 



d) 定义损失函数为$L(y,y^*)$ ;其中$y*$ 为样本的真实$y$值



## 误差反向

a) 记损失函数L关于第$i$ 层神经元的输出$z^i$ 的偏导为$\delta^i = \frac {\partial L} {\partial z^i}  \ \ \ (1)$

b) 首先我们来看看损失函数L在最后一层参数上的偏导;也就是$\frac {\partial L} {\partial W^{n-1}}$ 和 $\frac {\partial L} {\partial b^{n-1}}$
$$
\begin{align*}
& \frac {\partial L} {\partial W^{n-1}_{i,j}} \ \ \  \\
& = \frac {\partial L} {z^n_j} * \frac {\partial z^n_j} {\partial W^{n-1}_{i,j}}  \ \ \ (2)  \ \ \ \ //W^{n-1}_{i,j}是第n-1层的第i个神经元和第n层的第j个神经元的连接，所以只有z^n_j的误差经过W^{n-1}_{i,j}反向传播 \\
& =  \delta_j^n * \frac {\partial (\sum_{k=1}^{|l_{n-1}|}z^{n-1}_k W^{n-1}_{k,j}+b^{n-1}_j)} {\partial W^{n-1}_{i,j}} \ \ \  (3) \ \ \  //z^n_j是z^{n-1}这个行向量与权重矩阵W^{n-1}的第j列向量的乘积,加上偏置b^{n-1}_j\\
& = \delta_j^n * z^{n-1}_i \ \ \ \ (4)
\end{align*}
$$



对等式(4)一般化的向量表示为：
$$
\frac {\partial L} {\partial W^{n-1}} = (z^{n-1})^T \delta^n_j  \tag 5
$$

同理可得：
$$
\frac {\partial L} {\partial b^{n-1}} =\delta^n  \tag 6
$$


c) 更一般的损失函数L关于第$l$层(这里没有用索引$i$,避免跟等式1~4中的索引名相同，引起理解障碍)的参数上的偏导，也就是$\frac {\partial L} {\partial W^l}$ 和 $\frac {\partial L} {\partial b^l}$
$$
\frac {\partial L} {\partial W^l} = \frac {\partial L} {\partial z^{l+1}} * \frac {\partial z^{l+1}} {\partial W^l}    \ \ \ \ (7)  \\
= (z^l)^T\delta^{l+1}  \ \ \ \ (8)
$$
同理可得：
$$
\frac {\partial L} {\partial b^l} =\delta^{l+1} \tag 9
$$
d) 现在我们来看a)中定义的损失函数L关于第$l$层输出的偏导$\delta^l = \frac {\partial L} {\partial z^l}$
$$
\begin{align*}
&\delta^l_i=\frac {\partial L} {\partial z^l_i}  \ \ \   \\
&=\frac {\partial L} {\partial z^{l+1}} * \frac {\partial z^{l+1}} {\partial z^l_i}  \ \ \  (10)  \ \ \ \ //导数的链式法则\\
&=\frac {\partial L} {\partial z^{l+1}} * \frac {\partial (z^lW^l + b^l)} {\partial z^l_i} \ \ \  (11)  \ \ \ \ //z^l的定义\\
&=\sum_{j=1}^{|l_{l+1}|} \frac {\partial L} {\partial z^{l+1}_j} *  \frac {\partial (\sum_{k=1}^{|l_l|}z^l_k W^l_{k,j}+b^l_j)} {\partial z^l_i} \ \ \  (12)  \ \ \ \ //第l+1层的每个节点都有梯度专递到l层的第i个节点\\
&=\sum_{j=1}^{|l_{l+1}|} \delta^{l+1}_j * W^l_{i,j} \ \ \  (13)  \ \ \ \ //只有k=i时有梯度\\
&=\delta^{l+1} ((W^l)^T)_i  \ \ \  (14)  \ \ \ \ // 可以表示为l+1层梯度的行向量与权重W^l 的转置的第i个列向量的乘积

\end{align*}
$$
一般化的表示如下：
$$
\delta^l = \frac {\partial L} {\partial z^l} \\
=\delta^{l+1}(W^l)^T \ \ \ (15)
$$

### 结论

​        以上的证明就是为了说明下面的结论，所以请牢记以下的结论，后续自己写神经网络的反向传播都会用到以下结论

a) 根据公式15，损失函数L关于第$l$层神经元的偏导，就是第$l+1$ 层的偏导乘上第l层权重矩阵的转置。我们将公式(15)递归一下,$\delta^l=\delta^{l+1}(W^l)^T=\delta^{l+2}(W^{l+1})^T(W^l)^T=\delta^n(W^{n-1})^T(W^{n-2})^T...(W^l)^T \ \ (16)$

​    也就是说，只要求得损失函数L关于最后一层(n)的偏导$\delta^n$,其它任意层的偏导，直接用最后一层的偏导逐层乘上权重的转置即可。很简单，很对称，有木有？



b) 根据公式(8) ,损失函数L关于第$l$层权重$W^l$ 的偏导为，第$l$ 层输出的转置乘第$l+1$ 层的偏导$\frac {\partial L} {\partial W^l} = (z^l)^T\delta^{l+1}  \ \ \ \ (8)$



c) 根据公式(9) ,损失函数L关于第$l$层偏置$b^l$ 的偏导,就是第$l+1$ 层的偏导: $\frac {\partial L} {\partial b^l} =\delta^{l+1}  \ \ \ (9)$

​         由以上可知对任意的全连接层，我们只需要只到它后一层的偏导，就可以求得当前层参数(权重、偏置)的偏导; 就可以使用梯度下降算法更新参数了 
$$
w = w - \eta * \frac {\partial L} {\partial w}             \ \ //\eta为学习率 \tag {17} 
$$

$$
b = b - \eta * \frac {\partial L} {\partial b}  \tag {18}
$$

​          根据公式(16), 损失函数L关于任何一层的偏导,只需要求导损失函数关于最后一层的偏导$\delta^n$ 即可。$\delta^n$ 与损失函数的定义有关，下一节介绍常用损失函数的偏导计算。



## 常用损失函数

​         以下说明损失函数的偏导的计算

### 均方差损失

​          对于单个样本$(x,y*)$ ，定义如下：

$L(y,y*) = \frac 1 2(y-y^*)^2 \tag {19}$

​          其中$y$是神经网络最后一层的输出$y=z^n$ ,就是预测值
$$
\begin{align}
&\frac {\partial L} {\partial y_i} = \frac {\partial (\frac 1 2(y_i - y^*_i)^2)} {\partial y_i} \\ 
&=(y_i - y^*_i) * \frac {\partial (y_i - y^*_i)} {\partial y_i} \\
&=(y_i - y^*_i)    \ \ \ \ \ \ \ (20)
\end{align}
$$
​         更一般的表示为 $\frac {\partial L} {\partial y} = y - y^*$ ; 也就是$\delta^n=\frac {\partial L} {\partial y}=y-y^* = z^n-y^*      \tag {21}$

​         即使用均方误差情况下，损失函数L关于网络最后一层的导数就是预测值减实际值

### 交叉熵损失

​         交叉熵用于度量两个概率分布的差异;一般使用交叉熵损失前，会对网络输出做softmax变换进行概率归一化；所以我们这里介绍的交叉熵损失是带softmax变换的交叉熵。

​          softmax变换定义如下：
$$
a_i=e^{y_i}/\sum_k e^{y_k}  \tag {22}
$$
​          交叉熵损失定义如下：
$$
L(y,y^*) = - \sum_i y^*_i \log a_i \tag {23}
$$
a) 我们先来求$a_i$ 关于$y_j$ 的偏导
$$
\begin{align*}
&\frac {\partial a_i} {\partial y_j} = \frac {\partial(e^{y_i}/\sum_k e^{y_k})} {\partial y_j} \\
&= \frac {\partial e^{y_i}} {\partial y_j} * \frac {1} {\sum_k e^{y_k}} +  e^{y_i} * \frac {-1} {(\sum_k e^{y_k})^2} * \frac {\partial (\sum_k e^{y_k})} {\partial y_j} \\
&= \frac {\partial e^{y_i}} {\partial y_j} * \frac {1} {\sum_k e^{y_k}} -   \frac {e^{y_i}} {(\sum_k e^{y_k})^2} * e^{y_j} \\
&=\begin{cases}
\frac {e^{y_j}} {\sum_k e^{y_k}} - \frac {(e^{y_j})^2} {(\sum_k e^{y_k})^2}  &  i=j \\
-\frac {e^{y_i}e^{y_j}} {(\sum_k e^{y_k})^2}  & i\neq\ j
\end{cases} \\
&=\begin{cases}
a_i(1-a_i)  &  i=j  \\
-a_ia_j  & i\neq\ j  \tag {24}
\end{cases} 
\end{align*}
$$


b) 然后我们来求L关于$y_j$ 的偏导
$$
\begin{align}
&\frac {\partial L} {\partial y_j} = - \sum_i\frac {\partial( y^*_i \log a_i )} {\partial a_i} * \frac {\partial a_i} {\partial y_j} \\
&=- \sum_i \frac {y^*_i} {a_i} * \frac {\partial a_i} {\partial y_j} \\
&= - \frac {y^*_j} {a_j} * a_j(1-a_j) + \sum_{i \neq\ j}  \frac {y^*_i} {a_i} * a_ia_j & //注意这里i是变量,j是固定的 \\
&=-y^*_j(1-a_j) + \sum_{i \neq\ j} y^*_ia_j \\
&= - y^*_j + \sum_iy^*_i a_j  & //所有真实标签的概率之和为1\\
&=a_j - y^*_j
\end{align}
$$
​     更一般的表示为 :

$\frac {\partial L} {\partial y} = a - y^*  \tag {25}$

​        所以使用带softmax变换的交叉熵损失函数，损失函数L关于网络最后一层的导数就是预测值经softmax变换后的值减去真实值。(是不是跟均方差损失函数很相似,注意还是有差异的噢)